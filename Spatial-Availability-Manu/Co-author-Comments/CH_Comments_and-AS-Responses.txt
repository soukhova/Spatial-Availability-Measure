We note the opportunities are indivisible, yet we work in fractions when allocating opportunities. A larger conceptual shortcoming?
-	You are right, our measure allocations opportunities proportionally so by definition it does divide opportunities – but that doesn’t mean that those opportunities themselves are considered ‘indivisible’ (it is confusing to call them that though)

-	The opportunities are indivisible in the sense that they are only allocated X-to-Y (where X is 100% of ALL opportunities and the other Y is 100% of ALL population). X can equal Y or X doesn’t need to equal Y – either works. But the focus of ‘indivisible’ is a description of the type of opportunity. We made this more clear in the manuscript.
Line 276 in my revised .rmd (in R) – “To illustrate the analytical framework,” was left hanging. I deleted it.
-	Thanks!
The r and alpha parameters make sense, but part of me also feels like they over-complicate things. One alternative could be to introduce the raw population allocation factor (with accompanying equation), and then say how it could be expanded to different job types and different weights with a second equation. Just thinking we want mass adoption of this and many people in policy and planning - and indeed academia - have a tough time just grasping straight-up gravity measures.
-	I have discussed with Antonio and we think we should leave it in for now. Especially since the use-cases use them.
At line 322 in my revision we have “assume that the impedance function is a negative exponential function and assume that $\beta$.”
-	Ah yes – thank you for catching
-	Fixed 
The sentence on Line 344 “It is necessary to combine both population and travel cost factors to better reflect demand; these two components are in line with how demand is conventionally modelled in accessibility calculations which are re-scaled on a per demand-population basis or also consider competition [e.g., @allen2019; … ” was a bit confusing to me, and the grammar seems off.
-	Thank you for catching!
-	Fixed
Line 396 - Should note the numbers you talk about for the TTS are after weights are applied
-	You mean weights that make the values representative of the working/jobs population in the GGH (i.e. weights applied by TTS)?
-	I assume yes – and have added the following:
-	“The TTS data is based on a representative sample of between 3% to 5% of households in the GGH and is weighted to reflect the population covering the study area has a whole [@data_management_group_tts_2018]."
Line 398 - For R5R, the results when routing by car (or anything non-transit) are not sensitive to start time. I changed some of this text.
-	Thanks!
In the first set of maps, the scale starts at zero. This struck me as a bit troubling as it seems much of the GGH has zero spatial availability to jobs outside of Toronto. Maybe a “jenks” map would better highlight the variation rather than equal intervals?
-	I removed the Jobs in Toronto maps – so the problem now vanishes. I elaborate on this a few comments down.
Likewise, all the greys in the maps – no doubt due to only calculating for TAZs that have population >0 in the availability measure. But these zone are also excluded from the traditional accessibility analysis. We should probably note how the inputs are being used. Maybe all zeros are just shaded grey – are no population zones excluded from the start? Or are they included? If so, would these zones not naturally fall out of the equations when their population of 0 enters the weighting? I guess the key question is whether any population zones actually have an availability of zero in the calculations, and if so – why (network errors perhaps). And then if yes, do we want to distinguish between zones that have zero availability due to no population vs zero availability for the population due to impedance and competition.
-Great suggestion, I modified to reflect this clarification
I must admit, I got totally tripped up by the Toronto then GGH analyses, as the maps take on a similar format. I might suggest that we only present one set of results, but at different map scales. For example, you could just set the ggplot to map at the scale of the TAZs that make up the City of Toronto and leave the rest to not be rendered. This will have a nice level of zoom and would show the spatial variation better than the full + inset map strategy. Doing it for the main data would also get rid of all the zeros I mentioned above. I am not entirely sure the rationale of this first analysis anyhow… you are just using any destinations in the City of Toronto? The power of spatial availability to me is rooted in its link with reality, but this kind of breaks that link by focusing on the city of Toronto boundary. Do the availaibility relationships you want to show in the Discussion section not hold if you only focus on the second regional-scale analysis?
-	An amazing suggestion! So I decided to remove the Toronto scale analysis in the results… it really was increasing the complexity of the discussion for not too much gain of information. 
-	I did reformat the discussion though, and added a ‘difference’ plot with the full GGH  vs. Toronto-only jobs. I don’t include the exact numerical values (only Quantiles) – as it is only used to discuss how the whole GGH periphery TAZ flip from overestimated (in Toronto only jobs) to underestimated when including all jobs. This shows the impact that multiple-counting of opportunities has at INFLATING accessibility and shows how competition can help balance the measure.  When viewing these periphery areas through accessibility we are seeing them as more job access deprived than they may actually be (because of competition!) 
Sort of similar feedback on the main Figure 6. The GGH is a huge region. You could likewise tell the ggplot to render at the spatial scale of the GTHAregional municipalities. This would zoom in more on what are pretty tiny figures (no real fault of your own – this is one of my chief criticisms with figures in knit pdf documents using this Elsevier template – the maps get microscopic!). Would show more of the spatial variation I assume we are interested in vs say spatial availability up in cottage country. The TAZ polygons are pretty wonky up there too, e.g. Lake Simcoe.
-	I added an inset map of the Toronto+ region – think this fixes the focus. The spatial variation seen in the periphery of the GGH is interesting with spatial availability (particularly since there is no variation seen in accessibility) for this reason I want to show the full GGH. However, a zoom in into the GTA is needed, hence the rejigged figure.
I am not a big fan of the colour scheme used on the later difference figures, sorry. In this case, I would suggest a typical graduated colour scheme rather than blue to green to orange to red. Or even a diverging colour scheme – red to white (at zero) to blue. I am going to attach a pdf of a paper Antonio and I are revising that compares accessibility results for healthcare. It is mid revisions so there’s some loose ends in there! But I have all the code for stuff if you want anything - I use tmap though! In fact, I wonder if the same type of pairs plot in that paper could add value to this analysis, rather than some of the maps that could be condensed through a diverging colour scheme.
-	I changed to red to light-pink for overestimated – hot!- and blue to light-blue for underestimated -cold!-. Since the Toronto analysis was cut, I still think it’s worth keeping both over-estimated and underestimate GGH difference figures in the discussion – I believe it’s valuable to see their stark differences. 
Later in the discussion you talk about over-estimation using negative numbers. The signs and explanation don’t really match. Maybe the other way is to just divide access by availability or something, then you would get big numbers like… accessibility overestimates jobs on average by 6 times, or 600%, that kind of thing, rather than by -100%. Note that you are also talking in proportions rather than raw differences in this section. The mention of Moran’s I snuck up too – you could better explain it, that after running the Moran’s I we find this many are clustered?
-	 I decided to change my measure of ‘difference’ to be simply the proportion of (accessibility:spatial availability) – it is more intuitive! 
-	I chatted with Antonio and we decided to just removed Moran’s I. Since the purpose of this paper is to introduce this new measure – diagnostics of the trends in the data (even as measure by the measure) is not the focus, especially since it is confusing. 

I feel like we might need to revisit some of the conclusions too – it starts to wander into the MAUP, introducing it as an issue for FCA approaches and then commenting on availability. If we want to stick to this argument, it should be brought up earlier in the paper when talking about shortcomings of the existing methods.
-	The conclusion has been dramatically cut down so no MAUP
Getting to the end now – the paper is long. My copy and paste of the main body - before references and appendix – into Word has about ~10,000 words already. Not sure what the limit is for JTRG, but typically a papers is 7,500 to 8,000 words, inclusive of references. In this light, maybe some streamlining is in order to reduce some of the empirical results to just their core (e.g. one analysis vs two), perhaps even the additional use cases section.
-	Antonio and I chatted about this – he suggests that word limit is an editorial decision. If we tell a sharp story than perhaps they’ll accept it as it/let us know what they think is *yawn*. All your excellent suggestions have made it shorter and more defined, woohoo!
