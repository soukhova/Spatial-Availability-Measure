---
title: "Reading 4: Spatial Interaction Analysis"
author: 
- name: Antonio PÃ¡ez
  # Enter your name here:
- name: My Name 
subject: "ENVSOCTY 3LT3 Transportation Geography"

# Do not edit below this line unless you know what you are doing
# --------------------------------------------------------------
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    # The project-template-default.tex file was heavily  adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: reading-template-default.tex
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

NOTE: This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Transportation is a networked phenomenon. Network analysis provides a set of tools to investigate transportation systems as networks. Network analytical issues include location analysis, spatial interaction, accessibility, and connectivity. In this reading, we will introduce some elements of network analysis using the `R` statistical computing language.

To use this note you will need the following:

* The following data objects (available in your course package)
    + `hamilton_graph`
    + `hamilton_taz`
    + `hamilton_taz_employment`
    + `hamilton_taz_ph`
    + `hamilton_taz_travel`
    + `od_full`

Some of these objects you have already encountered in your previous readings and exercises. You previously used the `hamilton_graph` and `tbl_graph` objects to learn about network analysis in Reading 3 ('Network Analysis in `R`') and to examine the potential for mobility and accessibility in Exercise 3. You will recall that this object includes a portion of the road network in the Hamilton Census Metropolitan Area (CMA), sourced from OpenStreetMaps. Table `hamilton_taz` is a simple features object with the boundaries of traffic analysis zones in the Hamilton CMA. You worked with this in Reading 2 ('Geospatial Visualization for Transportation Trends using `R`') and Exercise 2 to visualize trip-making patterns in the region. 

Table `od_full` will be new to you. This data object contains an _origin-destination_ table. An origin-destination table looks  superficially similar to a table of _links_ in a network: it has identifiers for origins (from) and destinations (to), coordinates for the origins and destinations, a unique identifier for the origin-destination pair, and importantly the number of trips between the origin and destination. 

In this particular instance, the table includes trips for two distinct purposes (i.e., _work_ trips and _shopping_ trips) by origin-destination pair. While this is in fact a network, unlike the road network in `hamilton_graph`, it is not a physical one; instead, it is a point-to-point notional network with all the possible origins connected to all possible destinations by means of the number of trips, which in many cases is zero, since a vast number of zones do not actually have trips between them! The origin-destination table, like the traffic analysis zones, was sourced from the Transportation Tomorrow Survey, and corresponds to the year 2016.

## Preliminaries

Here we proceed to load the packages and data used in this reading. Before starting, however, it is useful to make sure that we have a clean Environment. The following command will remove all data and values currently in the Environment.
```{r}
rm(list = ls())
```

These are the libraries needed for the reading (some may need to be installed before they are available for loading):
```{r message=FALSE}
library(envsocty3LT3) # Course package
library(igraph) # Package for network analysis
library(scales) # This package provides scales functions for visualization 
library(sf) # Package to work with geospatial information in simple features format
library(tidygraph) # Package to work with spatial networks
library(tidyverse) # Family of packages for data manipulation, analysis, and visualization
library(units) # Package for working with units in `R`
```

We next invoke the data objects:
```{r}
data("hamilton_graph")
data("hamilton_taz")
data("hamilton_taz_employment")
data("hamilton_taz_ph")
data("hamilton_taz_travel")
data("od_full")
```

Remember that you can always consult the documentation for a data object in the course package by running `?<name of obect>` in the console; for example, `?od_full`. Here we quickly inspect the data objects that we loaded, beginning with the `tbl_graph` object `hamilton_graph`:
```{r}
hamilton_graph
```

This object includes the road network in `tbl_graph` format that you used in Reading 3 and Exercise 3. It is worth recalling that objects of class `tbl_graph` are also of class `igraph` which allows us to use package `igraph` to conduct many different types of network analyses - including finding shortest paths. The two main elements of a `tbl_graph` object are one table with information about the nodes and another table with information about the edges. Each of these elements can be _activated_ using the function `tidygraph::activate()` as desired.

The next object that we will inspect is the zoning system. The object `hamilton_taz` is a simple features object with the boundaries of the traffic analysis zones in Hamilton. Traffic analysis zones are used to report travel data collected by means of travel surveys. Together with the road network, the traffic analysis zones provide the necessary geographical information to study spatial interaction. 

The following chunk plots the road network and the traffic analysis zones:
```{r}
ggplot() + # Create blank ggplot object
  geom_sf(data = hamilton_graph %>% # To plot a `sf` object we use `geom_sf()`, and for this plot we want to use `hamilton_graph_local`
            activate(edges) %>%  # Activate the edges of the `tbl_graph` object; After activating the edges, the object will behave as an `igraph` or network object
            as_tibble() %>% # Convert the `igraph` object into a data frame
            st_as_sf(), # Convert the table into a simple features object
          color = "darkgray") + # Plot roads in dark gray
  geom_sf(data = hamilton_taz, # Plot the `sf` object; here we want to use `hamilton_taz`
          size = 1, # Change the thickness of the zone boundaries for ease of visualization
          fill = NA) # Do not fill the polygons with color; alternatively, transparent polygons 
```

As you can see, the road network is considerably more detailed than the zoning system. Every intersection is a node in the network, but not every intersection is a meaningful destination. But even for the zones this is true: not every point in the zone can be treated as a potential origin or destination (this would soon produce an overwhelming number of origins and destinations!). To deal with the difference in level of detail, it is customary to assume a center of mass, so to speak, for each zone. This can be the geometric _centroid_ of the zone (the mean of the coordinates of the polygon), or it can be a population-weighted centroid, which would tend to displace the geometric centroid towards the center of population (the weighted mean of the coordinates, with the weights given by population counts).

In this way, we can consider the zoning system as a collection of representative points, with one point per zone. Then, the zoning system is connected to the road network by finding, for each zone, the node in the network that is closest to its centroid.

Presently, the geometric centroid of the zones is considered, and there is a one-to-one identifier `GTA06` between the nodes in the network that are nearest to the centroid of a zone, and the identifier of the zone. The next chunk plots the road network and the traffic analysis zones; in this map, centroid nodes are red diamonds:
```{r}
ggplot() + # Create blank ggplot object
  geom_sf(data = hamilton_graph %>% # To plot a `sf` object we use `geom_sf()`, and for this plot we want to use `hamilton_graph_local`
            activate(edges) %>%  # Activate the edges of the `tbl_graph` object; After activating the edges, the object will behave as an `igraph` or network object
            as_tibble() %>% # Convert the `igraph` object into a data frame
            st_as_sf(), # Convert the table into a simple features object
          color = "darkgray") + # Plot roads in dark gray
  geom_sf(data = hamilton_graph %>% # To plot a `sf` object we use `geom_sf()`, and for this plot we want to use `hamilton_graph_local`
            activate(nodes) %>%  # Activate the nodes of the `tbl_graph` object; After activating the nodes, the object will behave as an `igraph` or network object
            as_tibble() %>% # Convert the `igraph` object into a data frame
            st_as_sf() %>% # Convert the table into a simple features object
            filter(!is.na(GTA06)), # Filter nodes that are _not_ NAs. This will select only those nodes that have a valid GTA06 identifier
          shape = 5, # Use diamonds for the points
          color = "red") + # Plot roads in dark gray
  geom_sf(data = hamilton_taz, # Plot the `sf` object; here we want to use `hamilton_taz`
          size = 1, # Change the thickness of the zone boundaries for ease of visualization
          fill = NA) # Do not fill the polygons with color; alternatively, transparent polygons 
```

The centroid nodes are representative of one traffic analysis zone each.

The next object that we will inspect is `od_full`. This is also a simple features object, with lines showing the connections between zone centroids. We can see the first few rows at the head of the table:
```{r}
head(od_full)
```

The table includes columns with zone identifiers for the origin and destination zones. The identifiers are the same as the columns `GTA06` in the nodes of the road network and the traffic analysis zones. This table is special: although information about trips can be found in the table `hamilton_taz_travel` (i.e., the `work_trips_` and `shop_trips_` columns), there the trips were total produced and attracted by the zones. The origin-destination table gives more detailed information about the trips by disaggregating those totals by origin zone and destination zone! This gives much richer information than just the totals and constitutes the core of spatial interaction analysis: it tells us precisely (within the context of the zoning system) where trips begin and where they end.

We see, for example, that zone 5077 did not _interact_ with zone 5030 (the first row in the table), since the number of work and shopping trips originating in zone 5002 and ending in zone 5001 was zero. In contrast, the following pair of zones did interact:
```{r}
od_full %>% # Pass `od_full` to the following function
  filter(Origin == "5066", # Filter rows where the value of column `Origin` is "5066"
         Destination == "5112") # Filter rows where the value of column `Destination` is "5122"
```

We can see that zone 5066 _produced_ 25 work trips that were _attracted_ by zone 5112, and also _produced_ 239 shop trips that were _attracted_ by zone 5112.

## Concepts of human interaction in space

Human interaction, be it migration, urban daily travel, marriage, information flows, and so on, has fascinated scholars for ages, and has been of interest for planning purposes to decision makers for almost as long. The conceptual foundations of the study of human interaction in space date back to at least the 19th century, when H.C. Carey wrote (as quoted in McKean, 1870):

>> Man, the molecule of society, is the subject of Social Science...The great law of _Molecular Gravitation_ (is) the indispensable condition of the existence of the being known as man...The greater the number collected in a given space, the greater is the attractive force that is there exerted...Gravitation is here, as everywhere, in the direct ratio of the mass and the _inverse_ one of distance

The fundamental idea is an analogy to Newton's [Law of Universal Gravitation](https://en.wikipedia.org/wiki/Newton%27s_law_of_universal_gravitation), originally published in PhilosophiÃ¦ Naturalis Principia Mathematica in 1687. Carey's insight was that principles similar to those in physics could be at work in human interaction. This insight, however, did not receive widespread attention, and the idea was only partially used by Ravenstein (1889) for the study of migration.

It was not until the 20th century that these ideas would be rediscovered. Here, again, there are parallel attributions, which vary by discipline. In this way, E.C. Young investigated in the 1920s the movement of farm people under the hypothesis that the relative volume of migration is proportional to the attractiveness of a destination and inversely proportional to the square of the distance to the destination. It is notable that Young introduced an empirical proportionality factor in this formulation of interaction (to ensure that the model's predictions matched observations of the world).

In the decade of the 1930s, economist W.J. Reilly proposed a law of retail gravitation that explained the point of indifference for two retail sites in terms of the attractiveness of the sites (i.e., bigger sites are more attractive than smaller sites), and the distance to each, with consumers generally having a preference to travel shorter distances, other things being equal. Accordingly:
$$
\frac{M_1}{d_1^2} = \frac{M_2}{d_2^2}
$$
where $P_i$ is the attractiveness (i.e., size) of retail site $i$ and $d_i$ the distance from an arbitrary point to the retail site.

It is easy to see that if we disregard size, the relationship is:
$$
\frac{d_1}{d_2} = 1
$$
This partial representation of Reilly's hypothesis suggests a connection to [Voronoi polygons](https://en.wikipedia.org/wiki/Voronoi_diagram), since the point of indifference will be on the boundary of the polygon, equidistant from the two sites. The full formulation, by extension, can be seen as an alternative way of expressing weighted Voronoi polygons.

Closer approximations to the expression of Carey where obtained shortly after these works.

Astrophysicist J.Q. Stewart, in a series of papers published in the decades of 1940 and 1950, posited a series of principles for so-called social physics, from a rule of inverse distance variation for social influence (1941) to a principle of demographic gravitation (1948). It is this latter work that most explicitly argues for a model of human interaction based on the physical law and put the principle in the notation that we currently use.

In Stewart's formulation, the gravity model for human interaction became:
$$
F = \frac{(\mu_1 N_1)(\mu_2N_2)}{d_{12}^2}
$$
where $F$ is the force of attraction between two population, $N_i$ is the size of the population of settlement $i$, $\mu_i$ is the _molecular weight_ of individuals in settlement $i$, and $d_{ij}$ is the distance between settlements $i$ and $j$. In the writing of Stewart (1948, p. 34), the molecular weight "...of the 'average American' is taken as unity. Presumably the molecular weight of an Australian aborigine, for example, is on this scale much less than one". This formulation, which to us now sounds shockingly racist, still manages to reflect the interest/ability of individuals to interact with others. For instance, the molecular weight could be purchasing parity, or reflect individual preferences for interaction (e.g., Australian aborigines might have little desire to interact with average Americans).

Around the same time as Stewart, sociologist GK Zipf published a series of papers testing regularities in the size of settlements, including what he termed the $\frac{M_1M_2}{D}$ hypothesis. The hypothesis was that movement between two communities with populations $M_1$ and $M_2$ separated by a shortest path of length $D$ is proportional to the ratio noted above. Zipf empirically showed that this regularity held for the case of intercity personal movement (1946), for goods movement by railways (1946), and information (1946).

Many elements of the social physics movement were later rejected by the social sciences, including the comparison to molecules and the value-laden assumptions about the molecular weight of individuals. Nonetheless, the gravity model continues to be widely used today in many applications, and the reason for this is that even if we reject the existence of "Laws" for human behavior, as a heuristic the gravity model still has remarkable power to capture regularities in behavior.

## Data preparation

To illustrate the analysis of spatial interaction we need to prepare some additional information. In particular, we want to add travel-related and employment information to the traffic analysis zones (this is something that you already did in Reading 2 and Exercise 2). The relevant tables are `hamilton_taz_travel` and `hamilton_taz_employment`. If you recall, we can _join_ these tables to `hamilton_taz` as follows:
```{r}
hamilton_taz <- hamilton_taz %>% # Pass `hamilton_taz` to the next function
  left_join(hamilton_taz_travel, # Join the table using "GTA06" as the key for the join
            by = "GTA06") %>%
  left_join(hamilton_taz_employment,
            by = "GTA06")
```

We are interested in two travel-related statistics: trips produced and trips attracted by each zone. We can select these variables and do a summary:
```{r}
hamilton_taz %>% # Pass `hamilton_taz` to the next function
  dplyr::select(work_trips_produced, # Select the columns named `work_trips_produced` and `work_trips_attracted`
                work_trips_attracted) %>% # Pass the selected columns to the next function
  summary() # Compute the summary statistics
```

From the summary, we see that the maximum number of work trips produced by any zone (`work_trips_produced`) is 2,837, and the maximum number of work trips attracted by any zone (`work_trips_attracted`) is 7,807. In addition to summary statistics, we can explore these variables by means of thematic maps (as you did in Exercise 2); here, the map is of `work_trips_produced`:
```{r}
ggplot() + # Create a blank ggplot object
  geom_sf(data = hamilton_taz, # Plot the `sf` object; here we want to use `hamilton_taz`
          aes(fill = work_trips_produced)) + # Make the fill color of the polygons a function of the number of work trips produced by zones
  scale_fill_distiller(palette = "YlOrRd", # Set the color palette for the fill colors of the polygons; use a yellow-orange-red palette 
                       direction = 1) # Set the direction of the palette so that darker colors correspond to higher values of the variable
```

Which zones produce large numbers of work trips? Do you notice a pattern? Repeat for `work_trips_attracted`. Which zones attract large numbers of work trips? Is there a pattern?

We will also create a variable called `total_employment`, which will be the sum of all types of jobs in a zone:
```{r}
hamilton_taz <- hamilton_taz %>% # Pass `hamilton_taz` to the following function
  mutate(total_employment = Jobs_Office_Clerical + # Mutate will add a new column to the table called `total_employment` which is the sum of Office/Clerical jobs...
           Jobs_Manufacturing_Construction_Trades + # Manufacturing/Construction/Trades jobs...
           Jobs_Professional + # Professional jobs...
           Jobs_Retail) #...and jobs in the Retail sector
```

We can verify that the trip information in the tables `hamilton_taz` and `od_full` is consistent by summarizing the trips in the origin-destination table:
```{r}
od_full %>% # Pass the object `od_full` to the next function
  st_drop_geometry() %>% # Drop the geometry of the object
  filter(Origin == 5002) %>% # Filter one origin to inspect
  select(Trips_work, Trips_shop) %>% # Select the columns with trips
  colSums() # Sum all values by column
```

The corresponding information in the `hamilton_taz` table is:
```{r}
hamilton_taz %>% # Pass the object `od_full` to the next function
  st_drop_geometry() %>% # Drop the geometry of the object
  filter(GTA06 == 5002) %>% # Filter one zone to inspect
  select(work_trips_produced, shop_trips_produced) # Select the columns with trip totals by zone of origin
```

Since `od_full` is a simple features object we can plot it as a geographical object. Here, the plot is for work trips:
```{r}
ggplot() + # Create a blank ggplot object
  geom_sf(data = od_full, # Plot the `sf` object; here we want to use `od_full`
          aes(color = Trips_work))
```

This plot is essentially a point-to-point network, but it is not very informative. For starters, the trips do not actually move in _straight lines_ from origin to destination, but rather move on the road network. This kind of plot is sometimes called a _desire lines_ plot. Another reason the plot is not very informative is that a large number of the lines are actually for zone pairs with zero trips!

This next chunk improves the plot in three ways: it includes the zoning system to provide more geographical context, it removes the lines that are for small number of trips, and modifies the lines (by changing their color, thickness, transparency) to enhance legibility:
```{r}
ggplot() + # Create a blank ggplot object
  geom_sf(data = hamilton_taz, # Plot the `sf` object; here we want to use `hamilton_taz`
          size = 1, # Change the thickness of the zone boundaries for ease of visualization
          fill = NA) + # Do not fill the polygons with color; alternatively, transparent polygons 
  geom_sf(data = od_full %>% # Plot the `sf` object; here we want to use `od_full`
            filter(Trips_work > 100), # Filter trips greater than 100
          aes(color = Trips_work, # The color of the lines is a function of the number of trips
              size = Trips_work, # The thickness of the lines is a function of the number of trips
              alpha = Trips_work)) +  # The transparency of the lines is a function of the number of trips
  scale_color_distiller(palette = "YlOrRd", # Set the color palette for lines; use a yellow-orange-red palette 
                        direction = 1) # Set the direction of the palette so that darker colors correspond to higher values of the variable
```

This makes it a little bit easier to discern the pattern of trips in Hamilton.

The next item that we need is the separation between origins and destinations (or the cost of movement between origin and destinations). Some relevant information is already included in the data provided, since the length of the links is an attribute of the edges in the `hamilton_graph` object. In addition, you have the tools (from Reading 3 and Exercise 3) to calculate the shortest path distances between origins and destinations. The shortest-path cost can be calculated by means of the `igraph` function `distances()`; the difference with the way you used this in Exercise 3 is that we do not want the shortest paths between _every_ origin and destination pair, but only between centroid nodes, since we are using these nodes to represent a traffic analysis zone:
```{r}
centroid_distances <- distances(hamilton_graph, # Calculate shortest path costs in a graph
                                v = hamilton_graph %>% # Choose the origin nodes in the graph for calculating distances
                                  activate(nodes) %>% # Activate the nodes
                                  filter(!is.na(GTA06)) %>% # Filter nodes that are _not_ NAs. This will select only centroid nodes, i.e., those with a valid GTA06 identifier 
                                  pull(nodeID), # Pull the node identifiers for the origins
                                to = hamilton_graph %>% # Choose the origin nodes in the graph for calculating distances
                                  activate(nodes) %>% # Activate the nodes
                                  filter(!is.na(GTA06)) %>% # Filter nodes that are _not_ NAs. This will select only centroid nodes, i.e., those with a valid GTA06 identifier 
                                  pull(nodeID), # Pull the node identifiers for the destinations
                                weights = hamilton_graph %>% # The weights will be the link lengths (i.e., distance)
                                  activate(edges) %>% # Activate the edges of the graph
                                  pull(length)) # Pull the length of the links
```

This is the summary of the inter-centroid shortest path distances:
```{r}
centroid_distances %>% # Pass the inter-centroid distances (a matrix) to the following function
  as.vector() %>% # Convert to vector
  summary() # Tabulate a summary
```

The longest network distance along a shortest path between two zones in Hamilton is 61.623 km. This is the diameter of the network according to length. Note that we could use different metrics for the separation between the zones, for instance time or cost. Time, in particular, is related to [distance by speed](https://www.bbc.co.uk/bitesize/topics/z83rkqt/articles/zhbtng8):
$$
s = \frac{d}{t}
$$
where $s$ is speed, $d$ is distance, and $t$ is time.

For the sake of the example, let us assume that speed is different in different types of links. These are the different types of links:
```{r}
hamilton_graph %>% # Pass `hamilton_graph` to the following functions
  activate(edges) %>% # Activate the edges of the `tbl_graph` object
  as_tibble() %>% # Convert to table
  pull(highway) %>% # Pull the column with information about the type of links
  factor() %>% # Convert to factor
  summary() # Calculate a summary
```

We can use `mutate()` to add a new column for travel time (see line 489 in Reading 3). While in Reading 3 we assumed that (walking) speed was constant for all link types, here we will assume that speed is different for different types of links. In this case, we will use the function `case_when()` to assign speeds to various cases:
```{r}
hamilton_graph <- hamilton_graph %>% # Pass the object `my_graph` to the next function
  activate(edges) %>% # Activate the nodes
  mutate(time = case_when(highway == "motorway" | 
                            highway == "motorway_link" ~ length/set_units(120, "km/h"), # When link is of type `motorway` or `motorway_link` speed is 120 km/h
                          highway == "primary" | 
                            highway == "primary_link" ~ length/set_units(110, "km/h"), # When link is of type `primary` or `primary_link` speed is 110 km/h
                          highway == "secondary" | 
                            highway == "secondary_link" ~ length/set_units(100, "km/h"), # When link is of type `secondary` or `secondary_link` speed is 100 km/h
                          highway == "tertiary" | 
                            highway == "tertiary_link" ~ length/set_units(90, "km/h"), # When link is of type `tertiary` or `tertiary_link` speed is 90 km/h
                          highway == "residential" | 
                            highway == "residential_link" ~ length/set_units(60, "km/h"))) # When link is of type `residential` or `residential_link` speed is 60 km/h
```

Find the shortest path travel times between zone centroids:
```{r}
centroid_times <- distances(hamilton_graph, # Calculate shortest path costs in a graph
                            v = hamilton_graph %>% # Choose the origin nodes in the graph for calculating distances
                              activate(nodes) %>% # Activate the nodes
                              filter(!is.na(GTA06)) %>% # Filter nodes that are _not_ NAs. This will select only centroid nodes, i.e., those with a valid GTA06 identifier 
                              pull(nodeID), # Pull the node identifiers for the origins
                            to = hamilton_graph %>% # Choose the origin nodes in the graph for calculating distances
                              activate(nodes) %>% # Activate the nodes
                              filter(!is.na(GTA06)) %>% # Filter nodes that are _not_ NAs. This will select only centroid nodes, i.e., those with a valid GTA06 identifier 
                              pull(nodeID), # Pull the node identifiers for the destinations
                            weights = hamilton_graph %>% # The weights will be the link lengths (i.e., distance)
                              activate(edges) %>% # Activate the edges of the graph
                              pull(time)) # Pull the length of the links
```

This is the summary of the inter-centroid shortest path distances:
```{r}
centroid_times %>% # Pass the inter-centroid distances (a matrix) to the following function
  as.vector() %>% # Convert to vector
  summary() # Tabulate a summary
```

The diameter of the network in Hamilton in travel time is 0.6265 h, or 37.6 minutes, approximately.

Once we have shortest path distances (or times, costs, etc., as desired), we need to add this information to the origin-destination table, so as to have all data items that we need for the analysis of spatial interaction in a single table. To do this, we need to create identifiers for the origin-destination pairs and copy the distances. In addition, we will convert the distances from meters to kilometers, and times from hours to minutes (this is just for ease of interpretation):
```{r}
centroid_costs <- expand.grid(Origin = hamilton_graph %>% # The function `expand.grid()` creates a table with all the combinations of values given in the inputs; the first input is the vector of zone identifiers of the origin nodes in the network used to calculate the distances
                                activate(nodes) %>% # Activate the nodes
                                #as_tibble() %>% # Convert to data frame
                                filter(!is.na(GTA06)) %>% # Filter nodes that are _not_ NAs. This will select only centroid nodes, i.e., those with a valid GTA06 identifier 
                                pull(GTA06), # Pull the zone identifiers
                              Destination = hamilton_graph %>% # The second input is the vector of zone identifiers of the destinations nodes in the network used to calculate the distances
                                activate(nodes) %>% #Activate the nodes
                                #as_tibble() %>%
                                filter(!is.na(GTA06)) %>%  # Filter nodes that are _not_ NAs. This will select only centroid nodes, i.e., those with a valid GTA06 identifier 
                                pull(GTA06)) %>%  # Pull the zone identifiers
  mutate(network_distance = centroid_distances %>% # Use `mutate()` to add a column with the inter-centroid shortest path distances 
           as.vector() %>% # Convert the inter-centroid shortest path distances from matrix to vector
           set_units("m"), # Set units to kilometers
         network_time = centroid_times %>% # Use `mutate()` to add a column with the inter-centroid shortest path distances 
           as.vector() %>% # Convert the inter-centroid shortest path distances from matrix to vector
           set_units("h") %>% # Set the units to hours
           set_units("min")) # Convert to minutes
```

The origin and destination zonal identifiers that we created above are necessary to join the shortest path costs to the origin-destination table, as shown below:
```{r}
od_full <- od_full %>% # Pass the object `od_full` to the next function
  left_join(centroid_costs, # Join to object `centroid_distances`
            by = c("Origin", 
                   "Destination")) # Join by matching values to both columns `Origin` and `Destination`
```

The final data pre-processing step is to join the attributes of the zones to the origin-destination table. To do this, we need to think about which attributes are of interest at the origin and which at the destination. In the current example, these are attributes that we think contribute to _producing_ trips at the origin, for instance population, number of dwellings, number of households, number of drivers, number of workers, and number of vehicles. Then, there are attributes that we think might contribute to _attracting_ trips at the destination, such as employment variables. All these variables are currently in the `hamilton_taz_ph` object, so we wish to copy them to the origin-destination table. To do this, we join the attributes that correspond to the origins:
```{r}
od_full <- od_full %>% # Pass object `od_full` to the next function
  left_join(hamilton_taz_ph, # Join to table `hamilton_taz_ph`
            by = c("Origin" = "GTA06")) # Join `od_full` based on column `Origin` to `hamilton_taz_ph` by column `GTA06`
```

And finally, we join the attributes corresponding to the destinations:
```{r}
od_full <- od_full %>% # Pass object `od_full` to the next function
  left_join(hamilton_taz %>% # Join to table `hamilton_taz`
              st_drop_geometry() %>% # Drop the geometry of `hamilton_taz`
              select(GTA06, Jobs_Manufacturing_Construction_Trades:total_employment), # Select from table `hamilton_taz` columns `GTA06` (the zone identifier), and then those ranging from `Pj_work` to `total_employment`
            by = c("Destination" = "GTA06")) # Join `od_full` based on column `Destination` to `hamilton_taz` by column `GTA06`
```

In this way, the table `od_full` has information about the size (i.e., mass) at the origin, the size (i.e., mass) at the destination, and the cost of travel between origins and destinations.

## The gravity model of spatial interaction

Spatial interaction is (at least in part) a function of the size (the mass effect) at the point of origin and the point of destination, as well as the cost to travel between the origin and the destination. We can term these the _size_ or _mass_ effects, and the _cost_ effect.

Originally, as noted above, these effects were expressed as an analogy to the physical law of gravitation, as follows:
$$
T_{ij} = k\frac{M_iM_j}{c_{ij}^2}
$$
where $T_{ij}$ is the interaction between origin $i$ and destination $j$, $k$ is an empirical constant, $M_i$ and $M_j$ are the masses at the origin and destination respectively, and $c_{ij}$ is the cost of moving between $i$ and $j$. Put in words, the amount of spatial interaction (e.g., the number of trips between $i$ and $j$) increases with the mass of $i$ and the mass of $j$, but decreases (rapidly - that is what the square does) with the cost of overcoming the space between $i$ and $j$.

The physical analogy to gravitation has been found to be somewhat rigid for human behavior (since humans do not behave like particles), but variations of the spatial interaction model above have been found to work well. A very general way of expressing similar principles for describing spatial interaction is as follows:
$$
T_{ij} = kM_i^{\beta_1}M_j^{\beta_2}f(c_{ij})
$$

The terms $\beta_1$ and $\beta_2$ help to modulate the size effects. For example, values of $\beta_1$ greater than 1 tend to make the size effect at the origin to grow more rapidly; same for values $\beta_2$ greater than 1 but for the destination. It is possible that a destination that is twice the size of another tends to attract _more_ than twice the number of trips: $\beta_2>1$ would capture growth that is more than proportional. This is something that we observe empirically. To see it, we can summarize the number of trips by different levels of, say, population at the origin, or total employment at the destination (as proxies for the masses).

First, we will find the _deciles_ of population and employment. The deciles are the values that split the distribution in ten equal parts:
```{r}
pop_deciles <- quantile(hamilton_taz_ph$population, # Find the quantiles of variable `population` in table `hamilton_taz`
                        seq(0, 1, 0.1)) # The quantiles will be a sequence of values between 0 and 1, in 0.1 intervals, i.e., the deciles 
                        #na.rm = TRUE) # Remove NAs when calculating the deciles

emp_deciles <- quantile(hamilton_taz$total_employment, # Find the quantiles of variable `total_employment` in table `hamilton_taz`
                        seq(0, 1, 0.1)) # The quantiles will be a sequence of values between 0 and 1, in 0.1 intervals, i.e., the deciles 
                        #na.rm = TRUE) # Remove NAs when calculating the deciles

pop_deciles
emp_deciles
```

In the following chunk, we will create new variables in the origin-destination table to classify the number of trips according to the population at the origin and the number of jobs at the destination; for this we will `cut()` the variables `population`, `total_employment`, and `network_distance` using the deciles, and add them to the table by means of `mutate()`:
```{r}
od_full <- od_full %>% # Pass object `od_full` to the next function; `mutate()` will create two new columns in the table: origin_mass_class and destination_mass_class
  mutate(origin_mass_class = cut(population, # Cut the variable population using breaks
                                 breaks = as.numeric(pop_deciles), # The breaks are the deciles of population; they need to be converted to numeric
                                 include.lowest = TRUE), # Include values of zero 
         destination_mass_class = cut(total_employment, 
                                      breaks = as.numeric(emp_deciles), # The breaks are the deciles of employment; they need to be converted to numeric
                                      include.lowest = TRUE)) 
```

Essentially, the chunk above created classes of mass. We can group the table by these classes, and then sum the trips and population. We will do this for work trips:
```{r}
trips_work_by_origin_mass <- od_full %>% # Pass object `od_full` to the next function
  st_drop_geometry() %>% # Drop the geometry of the object
  filter(Trips_work > 0) %>% # Filter all work trips greater than zero
  select(Trips_work, population, origin_mass_class) %>% # Select the columns `Trips_work`, `population`, and `origin_mass_class`
  group_by(origin_mass_class) # Group by `origin_mass_class`
```

The distribution of work trips by mass at the origin is given by the sum of work trips by class:
```{r}
tomd_work <- trips_work_by_origin_mass %>% # Pass the object `trips_work_by_origin_mass` to the next function
  summarize(Trips = sum(Trips_work), # Summarize the trips by group; the summarized variable is `Trips` and it is the sum of all `Trips_work` within a group
            population = sum(population), # Summarize the population by group; the summarized variable is the sum of all `population` within a group
            .groups = "drop") %>% # Drop the groups after summarizing
  mutate(frequency_trips = Trips/sum(Trips), # Use `mutate()` to create a `frequency_trips` variable: this is the proportion of trips out of all trips
         frequency_population = population/sum(population)) # create a `frequency_population` variable: this is the proportion of population out of total population
```

We are now ready to plot the distribution of trips and population by mass at the origin:
```{r}
tomd_work %>% # Pass object `tomd_work` to the next function
  pivot_longer(cols = starts_with("frequency_"), 
               names_to = "Variable",
               values_to = "frequency") %>%
  ggplot() + # Create a blank ggplot object
  geom_col(aes(x = origin_mass_class, # Use `geom_col()` to create a plot with bars; the x-axis is the mass classes 
               y = frequency, # The height of the bars is the frequency of trips produced by zones in the mass class
               fill = Variable), # The fill color of the columns will be different for frequency of trips and frequency of population
           position = "dodge") +
  xlab("population at the origin") +
  theme(axis.text.x = element_text(angle = 90)) # Rotate the labels of the x-axis to improve legibility 
```

The way to read the plot above is as follows: the ten percent of zones with the smallest populations (between 0 and 61.8 people) account for a tiny proportion of the population in Hamilton (<0.01%), and produce a negligible proportion of all work trips (about 0.3%); the next ten percent of zones (with populations between 61.8 and 349), contain more than one hundred times the proportion of the population (0.11%) but produce about 1.56% of all work trips (or about five times more than the proportion of the bottom ten percent of zones). The number of trips for the smaller zones grows more rapidly than the population. But the effect is not linear: bigger zones do produce more trips, but not proportionally more than their share of the population. This is noticeable in how the green columns (frequency of trips) are initially taller than the red columns (frequency of population), but then the red columns are bigger than the green. The top ten percent of the largest zones in Hamilton (with populations between 5,290 and 11,360 people) account for approximately 46.9% of the population of the city, but only about 27.8% of all trips.

We will repeat the analysis but now for the mass at the destination (proxied by `total_employment`). Here we group the table by the classes of mass at the destination (the deciles of total employment), and then sum the trips and total jobs in the group. We will do this for work trips:
```{r}
trips_work_by_destination_mass <- od_full %>% # Pass object `od_full` to the next function
  st_drop_geometry() %>% # Drop the geometry of the object
  filter(Trips_work > 0) %>% # Filter all work trips greater than zero
  select(Trips_work, total_employment, destination_mass_class) %>% # Select the columns `Trips_work`, `total_employment`, and `destination_mass_class`
  group_by(destination_mass_class) # Group by `destination_mass_class`
```

The distribution of work trips by mass at the destination is given by the sum of work trips by class of mass:
```{r}
tdmd_work <- trips_work_by_destination_mass %>% # Pass the object `trips_work_by_destination_mass` to the next function
  summarize(Trips = sum(Trips_work), # Summarize the trips by group; the summarized variable is `Trips` and it is the sum of all `Trips_work` within a group
            total_employment = sum(total_employment), # Summarize employment by group; the summarized variable is the sum of all `total_employment` within a group
            .groups = "drop") %>% # Drop the groups after summarizing
  mutate(frequency_trips = Trips/sum(Trips), # Use `mutate()` to create a `frequency_trips` variable: this is the proportion of trips out of all trips
         frequency_employment = total_employment/sum(total_employment)) # create a `frequency_employment` variable: this is the proportion of jobs in a zone out of total jobs in Hamilton
```

As before, we will plot the distribution of trips by total employment at the destination, next to the proportion of employment by class:
```{r}
tdmd_work %>% # Pass object `tomd_work` to the next function
  pivot_longer(cols = starts_with("frequency_"), 
               names_to = "Variable",
               values_to = "frequency") %>%
  ggplot() + # Create a blank ggplot object
  geom_col(aes(x = destination_mass_class, # Use `geom_col()` to create a plot with bars; the x-axis is the mass classes 
               y = frequency, # The height of the bars is the frequency of trips produced by zones in the mass class
               fill = Variable), # The fill color of the columns will be different for frequency of trips and frequency of population
           position = "dodge") +
  xlab("employment at the origin") +
  theme(axis.text.x = element_text(angle = 90)) # Rotate the labels of the x-axis to improve legibility 
```

The non-linear effect of mass at the destination is even more marked than the effect for mass at the origin in the previous figure: notice how the top ten percent of zones with the largest number of jobs (between 2,150 and 11,400 jobs) account for 78.7% of all jobs in Hamilton, and attract about 48% of all work trips in the city. This is indicative of the large concentration of employment in a relatively small number of zones in the city. This spatial organization of employment of course can have a large impact on commuting patterns.

There are numerous functions that have been used to represent the effect of cost. These functions typically capture some form of distance-decay. The basic inverse of the square of cost borrowed from physics is just one example. A more general formulation for a distance-decay function is the following:
$$
f(c_{ij})=c_{ij}^{-\beta_3}=\frac{1}{c_{ij}^{\beta_3}}
$$

This is called a negative power function, and depending on the value of $\beta_3$, the decay can be slow or fast, thus making the effect of cost on interaction weaker or stronger. The following plot illustrates various negative power functions:
```{r}
data.frame(cost = seq(from = 1, # Create a data frame with some made up values of cost. It does not really matter what these exact values are, because the objective is to illustrate the shape of the curves for different values of beta_3. Here, it is a sequence of cost from 1 to 5 in 0.1 intervals
                      to = 5, 
                      by = 0.1)) %>% # Pass the data frame with our made up values of cost to the next function
  mutate(ce0.5 = 1/cost ^ (1), # Use `mutate()` to create new columns for negative power of 1 (inverse distance decay), of 1.5, of 2, and of 2.5
         ce1.0 = 1/cost ^ (1.5),
         ce1.5 = 1/cost ^ (2),
         ce2.0 = 1/cost ^ (2.5)) %>% # Pass the data frame with the new columns to the following function
  pivot_longer(starts_with("ce"), names_to = "Coefficient", values_to = "effect") %>% # Pivot longer to collect the values of all functions in a single column
  ggplot() + # Create a blank ggplot object
  geom_line(aes(x = cost, # Draw lines using the cost (x-axis) and the effect (y-axis)
                y = effect, 
                color = Coefficient), # Color the lines by coefficient
            size = 1) + # Set the size of the lines
  ylab("cost effect") + # Change the label of the y-axis
  scale_color_discrete(name = parse_format()(c("beta[3]")), 
                       labels = c("1.0", "1.5", "2.0", "2.5")) # Change the labels in the legend for ease of reading the plot
```

We can see from the plot that smaller values of $\beta_3$ are associated with weaker effects of cost (the potential for interaction does not decay as rapidly), compared to larger values, which produce a stronger effect on interaction (i.e., the potential for interaction decays more rapidly).

## Calibration of the gravity model

Now the question is, given a gravity model of the form:
$$
T_{ij} = k\frac{M_i^{\beta_1}M_j^{\beta_2}}{c_{ij}^{\beta_3}} = kM_i^{\beta_1}M_j^{\beta_2}c_{ij}^{-\beta_3}
$$
what values of parameters $k$, $\beta_1$, $\beta_2$, and $\beta_3$ should be used in practice? To find appropriate values we need to _calibrate_ the gravity model. Calibration is the process of finding appropriate parameters based on observations of spatial interaction.

Take the log on both sides:
$$
log(T_{ij}) = log\big(k M_i^{\beta_1} M_j^{\beta_2}c_{ij}^{-\beta_3}\big)
$$
By the rules of logs:
$$
log(T_{ij}) = log(k) + log(M_i^{\beta_1}) +  log(M_j^{\beta_2}) + log(c_{ij}^{-\beta_3})\\
log(T_{ij}) = log(k) + \beta_1log(M_i) +  \beta_2log(M_j) - \beta_3log(c_{ij})
$$

Instead of the original multiplicative function that we had at the beginning, we have an additive function that can be estimated using regression analysis (see Reading and Exercise 2). Before doing so, it is important to note that the gravity model can and often is expanded to include more than two variables to represent the mass at the origin and at the destination. For example, the mass at the origin could include number of full- and part-time workers, since they may produce trips differently; the mass at the destination could include retail jobs and square footage of commercial development, for example. Similarly, a more generalized cost could include time and fares, for example.

Introducing additional variables in the gravity model is straightforward. Suppose that we have two variables of mass at the origin ($M_{1i}$ and $M_{2i}$), one at the destination ($M_{1j}$), and two cost variables ($c_{1ij}$ and $c_{2ij}$):
$$
T_{ij} = kM_{1i}^{\beta_1}M_{2i}^{\beta_2}M_{1j}^{\beta_3}c_{1ij}^{-\beta_4}c_{2ij}^{-\beta_5}
$$
Which after taking logs and reorganizing, give the following linear model:
$$
log(T_{ij}) = log(k) + {\beta_1}log(M_{1i}) + {\beta_2}log(M_{2i}) + {\beta_3}log(M_{1j}) - {\beta_4}log(c_{1ij}) - {\beta_5}log(c_{2ij})
$$

## Example of calibration

Here we calibrate a gravity model for work trips in Hamilton. 

In terms of data preparation, we need to transform the variables, since the model uses the log of the variables on the right hand side of the equation. Some of the variables have zeros for some zones. For example, a residential zone may have employment of zero. This poses a little bit of a problem, because the logarithm is not defined for zero; to solve this, we can add a very small constant to the variable to help us avoid the complications of calculating the log of zero. The constant needs to be sufficiently small to reduce the impact of this adjustment. In the chunk below, we add 0.001 to variables counted in units (of dwellings, households, workers, vehicles). In effect, we are adding 0.001 dwellings (households, etc.) to every zone:
```{r}
od_model <- od_full %>% # Copy `od_full` to a new object called `od_model` and pass it on to the next function
  filter(Trips_work > 0) %>% # Filter all zones with work trips greater than zero; equivalently, remove zone pairs that did _not_ have work trip between them
  mutate(log_population = ifelse(population == 0, log(population + 0.001), log(population)), # Use `mutate()` to transform the variables; this will create new columns in the table with the logged variables; `ifelse()` is useful to add a small constant _only_ if the value of the variable is zero
         log_House = ifelse(House == 0, log(House + 0.001), log(House)), 
         log_Apartment = ifelse(Apartment == 0, log(Apartment + 0.001), log(Apartment)),
         log_Townhouse = ifelse(Townhouse == 0, log(Townhouse + 0.001), log(Townhouse)),
         log_Inc_less_15k = ifelse(Inc_less_15k == 0, log(Inc_less_15k + 0.001), log(Inc_less_15k)),
         log_Inc_15k_to_40k = ifelse(Inc_15k_to_40k == 0, log(Inc_15k_to_40k + 0.001), log(Inc_15k_to_40k)),
         log_Inc_40k_to_60k = ifelse(Inc_40k_to_60k == 0, log(Inc_40k_to_60k + 0.001), log(Inc_40k_to_60k)),
         log_Inc_60k_to_100k = ifelse(Inc_60k_to_100k == 0, log(Inc_60k_to_100k + 0.001), log(Inc_60k_to_100k)),
         log_Inc_100k_to_125k = ifelse(Inc_100k_to_125k == 0, log(Inc_100k_to_125k + 0.001), log(Inc_100k_to_125k)),
         log_Inc_more_125k = ifelse(Inc_more_125k == 0, log(Inc_more_125k + 0.001), log(Inc_more_125k)),
         log_drivers = ifelse(drivers == 0, log(drivers + 0.001), log(drivers)),
         log_ft_workers =ifelse(ft_workers == 0, log(ft_workers + 0.001), log(ft_workers)),
         log_pt_workers = ifelse(pt_workers == 0, log(pt_workers + 0.001), log(pt_workers)),
         log_vehicles = ifelse(vehicles == 0, log(vehicles + 0.001), log(vehicles)),
         log_Jobs_Office_Clerical = ifelse(Jobs_Office_Clerical == 0, log(Jobs_Office_Clerical + 0.001), log(Jobs_Office_Clerical)),
         log_Jobs_Manufacturing_Construction_Trades = ifelse(Jobs_Manufacturing_Construction_Trades == 0, log(Jobs_Manufacturing_Construction_Trades + 0.001), log(Jobs_Manufacturing_Construction_Trades)),
         log_Jobs_Professional = ifelse(Jobs_Professional == 0, log(Jobs_Professional + 0.001), log(Jobs_Professional)),
         log_Jobs_Retail = ifelse(Jobs_Retail == 0, log(Jobs_Retail + 0.001), log(Jobs_Retail)),
         log_net_distance = log(network_distance))
```

As a sanity check, we will verify that there are no NAs, Inf, or other weird values in the table that might cause problems later on:
```{r}
od_model %>%
  filter(Trips_work > 0) %>%
  select(starts_with("log")) %>%
  summary()
```

The table looks clean, with no extraneous values.

Calibration of the gravity model uses regression analysis (see Reading 1, 'Linear regression in `R`'). There are two differences with the way you used regression analysis in Exercise 2:

1. In that exercise, you were introduced to simple linear regression (a regression with only one variable in the right hand side); now we have a multivariate regression model (a model with two or more variables on the right hand side); and

2. The regression model you saw before was for _continuous_ variables (variables that can take fractional values, essentially decimals); in the case of trips, it is convenient to treat them not as a continuous variable that potentially fractions of a trip (0.1 trip), but rather as counts, where trips can only take integer values (i.e., no decimals).

A model used for counts is called Poisson, and it is implemented in `R` as a _generalized linear model_ (or glm for short). The following chunk calibrates a gravity model for work trips using Poisson regression:
```{r}
# Use `glm()` for estimating a generalized linear model

mod_work = glm(Trips_work ~  # The dependent variable (on the left hand side of the equation) is the number of trips
                 # Mass variables at the origin
                 log_population + log_drivers + log_vehicles + # Population, and number of drivers and vehicles
                 log_ft_workers + log_pt_workers + # Number of full time and part time workers
                 log_Apartment + log_Townhouse + log_House + # Number of dwellings of different types
                 log_Inc_less_15k + log_Inc_15k_to_40k + log_Inc_40k_to_60k + log_Inc_60k_to_100k + log_Inc_100k_to_125k +  log_Inc_more_125k + # Number of households with different levels of income
                 # Mass variables at the destination
                 log_Jobs_Office_Clerical + log_Jobs_Manufacturing_Construction_Trades + log_Jobs_Professional + log_Jobs_Retail + # Number of jobs of different types
                 # Cost variables
                 log_net_distance, # Network distance
               family = poisson (link = log), # The model is `poisson` with a logarithmic link function
               data = od_model) # The data for estimating the model is in table `od_model`
```

Use `summary()` to print the results of the model:
```{r}
summary(mod_work)
```

We see that the parameters are significant for most variables (meaning they are significantly different from zero with probability as seen in $Pr(>|z|)$). Since the probabilities are so small, we are confident in saying that the estimated values of the parameters are different from zero. Why is this important? Since the parameters are the exponents of the mass variables and the cost variables, a parameter of zero would say that the variable does not affect the number of trips between origins and destinations (since any number to the power of zero is one):
$$
M_i^0=1
$$
With respect to the signs of the parameters, positive values translate into a _directly proportional_ relationship with trips (higher values are associated with greater interaction), whereas negative parameters translate into an _inversely proportional_ relationship with trips (higher values are associated with lower interaction). For example, the effect of population at the origin is as follows (the parameter is the power of the variable):
$$
T_{ij} \propto Pop^{0.283}
$$
We can visualize this effect by simulating a population variable ranging from 0 to 13,590 (approximately the population of the most populous zone in Hamilton):
```{r}
data.frame(population = seq(from = 0, # Simulate a population variable
                            to = 13590,
                            by = 10)) %>% # Simulate the variable in increments of 10 people
  mutate(mass_effect_population = population^mod_work$coefficients["log_population"]) %>% # Use `mutate()` to calculate the mass effect of population; extract the parameter of the population variable from the model object by means of `mod_work$coefficients["log_population"]`
  ggplot() + # Create a blank ggplot object
  geom_line(aes(x = population, # Plot a line; the x-axis is the population
                y = mass_effect_population)) # The y-axis is the mass effect of population
```

The mass effect of population at the origin increases rapidly for small populations, but then more slowly for large populations; in other words, larger populations do not produce proportionally more trips. For example, the smallest non-zero population in the plot (10 people) tends to increase the number of work trips by a factor of 1.917. In contrast, the largest population in the plot (13,590 people, that is 1,359 times more people than the smallest zone) tends to increase the number of work trips by a factor of only 14.738 (or less than eight times more than the smallest population).

We can also visualize the cost effect, which essentially means plotting the distance-decay function:
$$
f(c_{ij}) = c_{ij}^{-0.161} = \frac{1}{c_{ij}^{0.161}}
$$

In this following chunk we simulate distances between 0.1 km and 41 km (the diameter of Hamilton), in 100 meter increments, and then plot after calculating the distance-decay:
```{r}
data.frame(distance = seq(100, # Simulate a distance variable
                          61300, 
                          100)) %>% # Use 100 m increments
  mutate(f = distance^mod_work$coefficients["log_net_distance"]) %>% # Calculate the distance-decay function; retrieve the parameter of the distance variable from the model object by means of `mod_work$coefficients["log_net_distance"]`
  ggplot() + # Create a blank ggplot object
  geom_line(aes(x = distance, # Plot a line; the x-axis is the network distance
             y = f)) + # The y-axis is the distance-decay
  ylab(expression(f(distance[ij]))) # Set the y-axis label
```

We see that the potential for interaction declines quite rapidly for distances between 100 m and 5,000 m, and then more slowly after that. The effect on spatial interaction is as follows: other things being equal, at short distances the number of trips is close to proportional (since at a distance of 1 m the cost effect is essentially neutral, since 1 raised to any power is 1). The effect of greater distances is to reduce the potential for interaction to fractions of what it would otherwise be. For example, at a distance of 100 m, the number of trips is already multiplied by a factor of 0.477, and at a distance of 20,000 m the factor is 0.203 (or one fifth of what it would be at a distance of 1 m).

We can further examine the model by extracting the fitted values (the values predicted by the model) to compare them to the observed values:
```{r}
fitted_values_work <- data.frame(Trips_work = od_model %>% # Create a data frame with the observed number of work trips from the `od_full` table
                                   pull(Trips_work), # Pull variable `Trips_work`
                                 network_distance = od_model %>% # Also copy the network distances from the `od_full` table
                                   pull(network_distance), # Pull variable `network_distance`
                                 Trips_work_pred = mod_work$fitted.values) # Extract the predicted values from the model object via `mod_work$fitted.values`

fitted_values_work %>% # Pass object `fitted_values_work` to `select()`
          select(starts_with("Trips")) %>% # Select columns that start with "Trips"; this will keep the columns `Trips_work` and `Trips_work_pred`, the predicted values
  colSums() # Sum the columns
```

The total number of work trips in the table is 134,922 and the model predicts 134,922 trips (it is a property of the Poisson model that if it includes a constant the total of the predicted values matches the total of the observed values).

Plot observed versus predicted trips:
```{r}
ggplot(fitted_values_work, aes(x = Trips_work, y = Trips_work_pred)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1)
```

The model is not great at predicting some of the largest flows in the system (those with 100 or so trips and more). That said, we can compare the trip length distribution to see how the model performs from that perspective. To compare the predicted trip length distribution to the observed trip length distribution we need to add the network distance and distance classes (similar to what we did in the chunk in line 409 in this notebook):
```{r}
fitted_values_work <- fitted_values_work %>%
  mutate(cost_bin = cut(network_distance %>%
                          set_units("m") %>%
                          set_units("km"), 
                        breaks = seq(from = 0, to = 41, by = 1),
                        labels = sprintf("%d km",seq(1:41))))
```

Plot the observed and predicted trip length distributions:
```{r}
fitted_values_work %>% 
  select(Trips_work, Trips_work_pred, cost_bin) %>%
  group_by(cost_bin) %>%
  summarize(Trips_work = sum(Trips_work),
            Trips_work_pred = sum(Trips_work_pred),
            .groups = "drop") %>%
  pivot_longer(starts_with("Trips"), names_to = "Type", values_to = "Trips") %>%
  ggplot() + 
  geom_col(aes(x = cost_bin, 
               y= Trips, 
               fill = Type),
           position = "dodge") +
  xlab("distance") +
  scale_fill_discrete(name = "Distribution", 
                      labels = c("Observed", "Predicted")) + 
  theme(axis.text.x = element_text(angle = 90))
```

Despite failing to predict some of the biggest flows in the system, the model does a very reasonable job of replicating the observed trip length distribution.

## Application

The calibrated gravity model is informative about the trip-making behaviour of the public. Particularly, the distance-decay function tells us about how people respond to the friction of space. This information can be used in different ways. Here, we will illustrate how it is useful to calculate accessibility.

Recall that accessibility ($A$) is defined as the weighted sum of opportunities $W_j$ from the perspective of an origin $i$:
$$
A_i=\sum_{j=1}^NW_jf(c_{ij})
$$

The calibrated distance-decay function can be used in place of $f(c_{ij})$ to estimate the accessibility. In this example, we will calculate accessibility to jobs, and the measure of opportunities will be total employment, which is in the origin-destination table `od_full`.
```{r}
accessibility_to_jobs <- od_full %>% # Pass the object `od_full` to the next function; results will be saved to `accessibility_to_jobs`
  st_drop_geometry() %>% # Drop the geometry of the simple features object
  mutate(network_distance = drop_units(network_distance), # Drop the units of the network distances
         f = network_distance^mod_work$coefficients["log_net_distance"]) %>% # Calculate the distance decay effect based on the network distance between origins and destinations
  drop_na(total_employment) %>% #
  group_by(Origin) %>% # Group the table by `Origin`
  summarize(accessibility = sum(total_employment * f), # Accessibility is the weighted sum of employment at the destination; the weights are the values of the decay-function
            .groups = "drop") # Drop the groups after summarizing
```

The summary statistics of accessibility to jobs in Hamilton are as follows:
```{r}
summary(accessibility_to_jobs)
```

Accessibility to employment in Hamilton ranges from 48,784 jobs to 62,653 jobs. We can join the accessibility to the zoning system to map this result:
```{r}
hamilton_taz %>% # Pass object `hamilton_taz` to the following function
  left_join(accessibility_to_jobs, # Join table `accessibility_to_jobs` to `hamilton_taz`
            by = c("GTA06" = "Origin")) %>% # The keys for the join are "GTA06" (the taz identifier) on `hamilton_taz` and "Origin" (the taz identifier) on `accessibility_to_jobs`; pass to ggplot
  ggplot() + # Create a blank ggplot object with the input from the preceding lines
  geom_sf(aes(fill = accessibility)) + # Plot the simple features; the fill color of the polygons is a function of `accessibility`
  scale_fill_distiller(palette = "YlOrRd", # Select a yellow-orange-red color palette for the fill color
                       direction = 1) # The direction of the palette gives lighter colors to smaller values of accessibility
```

Accessibility to employment is higher in the central parts of Hamilton than in the periphery of the city. It is important to remember that this analysis includes only employment _in_ Hamilton, not that available in Oakville, let alone Toronto. It is possible that accessibility in the periphery of Hamilton would be higher if we included employment beyond the boundaries of the city.
